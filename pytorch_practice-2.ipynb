{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02919600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c70aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\dataset'\n",
    "dataset = datasets.ImageFolder(root=r'C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\dataset', transform=my_transforms)\n",
    "subdir = dataset.classes\n",
    "class_weights = []\n",
    "subdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sdir in subdir:\n",
    "    files = os.listdir(os.path.join(path, sdir))\n",
    "    class_weights.append(1/len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = [0]*len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197599c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([label for data, label in dataset])\n",
    "# for data, label in dataset:\n",
    "#     print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc798631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, batch_size):\n",
    "    my_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=root_dir, transform=my_transforms)\n",
    "    subdirectories = dataset.classes\n",
    "    class_weights = []\n",
    "\n",
    "    # loop through each subdirectory and calculate the class weight\n",
    "    # that is 1 / len(files) in that subdirectory\n",
    "    for subdir in subdirectories:\n",
    "        files = os.listdir(os.path.join(root_dir, subdir))\n",
    "        class_weights.append(1 / len(files))\n",
    "\n",
    "    sample_weights = [0] * len(dataset)\n",
    "\n",
    "    for idx, (data, label) in enumerate(dataset):\n",
    "        class_weight = class_weights[label]\n",
    "        sample_weights[idx] = class_weight\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        sample_weights, num_samples=len(sample_weights), replacement=True\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    loader = get_loader(root_dir=r\"C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\dataset\", batch_size=8)\n",
    "\n",
    "    num_retrievers = 0\n",
    "    num_elkhounds = 0\n",
    "    for epoch in range(10):\n",
    "        for data, labels in loader:\n",
    "            num_retrievers += torch.sum(labels == 0)\n",
    "            num_elkhounds += torch.sum(labels == 1)\n",
    "\n",
    "    print(num_retrievers.item())\n",
    "    print(num_elkhounds.item())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c41f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in train_loader:\n",
    "    print(data.shape, target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aee445",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "num_epochs = 3\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6809f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a tensor with requires_grad=True to track computation with it\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Define a function y = x^2\n",
    "y = x**2\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "y = x**3 + 2*x + 1\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)  # should output tensor([17.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # when loading file paths\n",
    "import pandas as pd  # for lookup in annotation file\n",
    "import spacy  # for tokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load img\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e064ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    loader, dataset = get_loader(\n",
    "        \"flickr8k/images/\", \"flickr8k/captions.txt\", transform=transform\n",
    "    )\n",
    "\n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        print(imgs.shape)\n",
    "        print(captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = r'C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\intel_image_class\\Flickr 8k Dataset\\captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6076627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(p)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_id = len(list(df['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = os.listdir(r\"C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\intel_image_class\\Flickr 8k Dataset\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc31eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in imgs_list:\n",
    "#     print(df[df['image'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e34453",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_id/len(imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(r'C:\\Users\\dines\\OneDrive\\Documents\\torch-nlp\\intel_image_class\\Flickr 8k Dataset\\images\\1000268201_693b08cb0e.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226b88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5db501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfd3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd58436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes)\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\dines\\Music\\Pytorch_image_classification\\Cricket_Legends\\Cricket_stars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min([len(os.listdir(os.path.join(path, i))) for i in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbeb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket_stars = os.listdir(path)\n",
    "for fld in os.listdir(path):\n",
    "    imgs_path = os.path.join(path, fld)\n",
    "    files = shuffle(os.listdir(imgs_path))\n",
    "    train_path = os.path.join(os.path.split(path)[0], 'train')\n",
    "    if not os.path.isdir(train_path):\n",
    "        os.mkdir(train_path)\n",
    "        [os.mkdir(os.path.join(train_path, i)) for i in cricket_stars]\n",
    "    [shutil.copy(i, os.path.join(train_path, fld)) for i in [os.path.join(imgs_path, i) for i in files[:100]]]\n",
    "    test_path = os.path.join(os.path.split(path)[0], 'test')\n",
    "    if not os.path.isdir(test_path):\n",
    "        os.mkdir(test_path)\n",
    "        [os.mkdir(os.path.join(test_path, i)) for i in cricket_stars]\n",
    "    [shutil.copy(i, os.path.join(test_path, fld)) for i in [os.path.join(imgs_path, i) for i in files[100:140]]]\n",
    "    val_path = os.path.join(os.path.split(path)[0], 'val')\n",
    "    if not os.path.isdir(val_path):\n",
    "        os.mkdir(val_path)\n",
    "        [os.mkdir(os.path.join(val_path, i)) for i in cricket_stars]\n",
    "    [shutil.copy(i, os.path.join(val_path, fld)) for i in [os.path.join(imgs_path, i) for i in files[140:]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6160af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb07c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a21887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b75d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1aa493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(\n",
    "            x, (h0, c0)\n",
    "        )  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e73f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(root='/datasets', train=True, transform=transforms.ToTensor() ,download=True)\n",
    "test = datasets.MNIST(root='datasets', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bac6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_load = DataLoader(test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_load:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f59a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "sequence_length = 28\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def clean_text(text ):\n",
    "    words = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    words = ' '.join([w for w in words if not w.isdigit() and len(w)>2])\n",
    "    return words.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2857774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca072285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingText(data, type='train'):\n",
    "    df = pd.read_csv(data)\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    df['len_text'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df['len_text']>2]\n",
    "    print('-------------------------')\n",
    "    print(df['sentiment'].value_counts())\n",
    "    # mask = df['len_text']>2\n",
    "    # df = df[mask]\n",
    "    train_seq_len = df['len_text'].max()\n",
    "    print(\"\\nLength of {} text: \".format(type), df['len_text'].max())\n",
    "    df['text'] = df['text'].apply(remove_emoji)\n",
    "    df['text'] = df['text'].apply(remove_url)\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    le = LabelEncoder()\n",
    "    df['labels'] = le.fit_transform(df['sentiment'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processingText(r\"C:/Users/dines/Music/Work/pytorch_practice/PyTorchTweetTextClassification/train.csv\")\n",
    "test_data = processingText(r\"C:/Users/dines/Music/Work/pytorch_practice/PyTorchTweetTextClassification/test.csv\", 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12951fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data['text'].tolist(), train_data['labels'].tolist(), test_size=0.2, stratify=train_data['labels'].tolist(), random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53901974",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "\n",
    "\n",
    "print('Valid data len:'+str(len(X_test)))\n",
    "print('Class distribution'+ str(Counter(y_test)))\n",
    "\n",
    "print('Test data len:'+str(len(test_data['text'].tolist())))\n",
    "print('Class distribution'+ str(Counter(test_data['labels'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train, y_train))\n",
    "val_data = list(zip(X_test, y_test))\n",
    "test_data = list(zip(test_data['text'].tolist(), test_data['labels'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a832c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8702c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_token(data):\n",
    "    for text, _ in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_token(train_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "# vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c181f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline('hi there how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_token(data):\n",
    "    for text, _ in data:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_token(train_data), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b331467",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "text_pipeline('are you there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4a6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc1 = nn.Linear(embed_dim,64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc3.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = F.relu(self.fc1(embedded))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a36d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassificationModel, self)\n",
    "        self.embedding = nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a23df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5356236",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\dines\\Music\\Work\\pytorch_practice\\imbalanced_dataset\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=path, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5174be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = []\n",
    "for subdir in dirs:\n",
    "    files = os.listdir(os.path.join(path, subdir))\n",
    "    class_weights.append(1/len(files))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = [0]*len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (data, labels) in enumerate(dataset):\n",
    "    class_weight = class_weights[labels]\n",
    "    sample_weights[idx] = class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\dines\\Music\\Work\\pytorch_practice\\imbalanced_dataset\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81259a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=path, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4479077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = []\n",
    "for sub_fold in folds:\n",
    "    files = os.listdir(os.path.join(path, sub_fold))\n",
    "    class_weights.append(1/len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = [0]*len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (data, label) in enumerate(dataset):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[idx] = class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba400fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=8, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c457bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrivers = 0\n",
    "num_elkonds = 0\n",
    "for epoch in range(10):\n",
    "    for data, label in loader:\n",
    "        num_retrivers += torch.sum(label == 0)\n",
    "        num_elkonds += torch.sum(label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0831c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elkonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b762e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 4, requires_grad=True)  # create a non-scalar tensor with requires_grad=True\n",
    "y = x**2 + 3*x + 1  # perform some operations on the tensor\n",
    "z = y.mean()  # compute the mean of the tensor to obtain a scalar-valued tensor\n",
    "grad_x = torch.autograd.grad(z, x)  # compute gradients of z with respect to x\n",
    "print(grad_x)  # print gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e799025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 4, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "z = y.mean()\n",
    "grad_x = torch.autograd.grad(z, x)\n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d56fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2 + 3*x + 1\n",
    "# y.mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91223cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[1, 2, 3],\n",
    " [4, 5, 6],\n",
    " [7, 8, 9],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f92d7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\dines\\Music\\Work\\pytorch_practice\\captcha-recognition-pytorch\")\n",
    "import dataset\n",
    "\n",
    "import torch\n",
    "from glob import glob\n",
    "import os, re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02a52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = glob(os.path.join(config.DATA_DIR, '*.png'))\n",
    "list_targets = [re.split(r\"\\\\|\\.\", img)[-2] for img in image_files]\n",
    "targets = [[c for c in target] for target in list_targets]\n",
    "flat_targets = [c for t in targets for c in t]\n",
    "le = LabelEncoder()\n",
    "le.fit(flat_targets)\n",
    "le_enc = [le.transform(t) for t in targets]\n",
    "le_enc = np.array(le_enc) + 1\n",
    "(\n",
    "    train_imgs,\n",
    "    test_imgs,\n",
    "    train_target,\n",
    "    test_target,\n",
    "    _,\n",
    "    test_target_origin) = train_test_split(image_files, le_enc, list_targets, train_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = dataset.ClassificationDataset(\n",
    "    image_paths=train_imgs,\n",
    "    targets=train_target,\n",
    "    resize=(config.IMAGE_HEIGHT, config.IMAGE_WIDTH)\n",
    ")\n",
    "\n",
    "train_load = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "562196ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 15.  2. 17.  3.]\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.transpose(train_dataset[0]['images'].numpy(), (1, 2, 0)))\n",
    "plt.show()\n",
    "print(train_dataset[0]['targets'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82c6a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in tqdm(train_load):\n",
    "#     for images, targets in data.items():\n",
    "#         print(images, targets)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88e722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85885e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk0 = tqdm(train_load, total=len(train_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5c507b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8332, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8332, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Define the loss function\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "\n",
    "# Define the input and target\n",
    "x = torch.randn(3, 10)\n",
    "y = torch.tensor([1, 3, 2])\n",
    "\n",
    "# Compute the loss using nn.CrossEntropyLoss\n",
    "output = Model()(x)\n",
    "loss1 = criterion1(output, y)\n",
    "print(loss1)\n",
    "\n",
    "# Compute the loss using nn.NLLLoss\n",
    "log_prob = nn.LogSoftmax(dim=1)(output)\n",
    "loss2 = criterion2(log_prob, y)\n",
    "print(loss2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1271a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6112, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "y = torch.tensor([1, 2, 3])\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "\n",
    "output = Model()(x)\n",
    "print(criterion1(output, y))\n",
    "\n",
    "nn.Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d50935c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2313,  0.6508,  0.4546,  0.6110, -0.0449],\n",
       "        [ 0.0241,  0.1521, -0.0199,  0.7970, -0.2015],\n",
       "        [-0.2117,  0.1482,  0.8465, -1.3221, -0.6621]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c75d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14,  4,  7,  1, 14]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(1, 20, (1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "76d10674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 18, 75])\n",
      "torch.Size([1, 64, 1350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1350, 64])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([1, 64, 18, 75])\n",
    "print(x.shape)\n",
    "x = x.view(1, x.size(1), -1)\n",
    "print(x.shape)\n",
    "x.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8ab86e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 18, 75])\n",
      "torch.Size([1, 75, 64, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 75, 1152])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([1, 64, 18, 75])\n",
    "print(x.shape)\n",
    "x = x.permute(0, 3, 1, 2)\n",
    "print(x.shape)\n",
    "x.view(1, x.size(1), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9c8ae2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6786416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "10f271e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 75, 300])\n",
      "torch.Size([1, 128, 37, 150])\n",
      "torch.Size([1, 64, 37, 150])\n",
      "torch.Size([1, 64, 18, 75])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a482abc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. I want one window with user changeable, based on requirement.\n",
    "2. I want 2 text fields along with 2 choose file buttons, text fields and choose file buttons sizes should be user changeable based on their requirement, I want these choose file buttons should be right side of every text field, whenever user choose any file, file should be visible in text fields.\n",
    "3. after those 2 text fields, I want separate line like (----------------------) for next text fields and other elements.\n",
    "4. after line separate, I want 4 more text fields with 4 more choose file buttons, sizes should be user changeable based on their requirement, I want these choose file buttons should be right side of every text field, whenever user choose any file, file should be visible in text fields.\n",
    "5. I want 3 different drop downs, I want left to right format, not top to down, size should be user changeable based on their requirement . \n",
    "6. 2 simple text areas with \"ex1\", \"ex2\", these should be left to right format, size should be user changeable based on their requirement . \n",
    "7. I need 2 buttons, one is \"Skip\" once user choose skip, window should be close without printing aything, other one is \"Submit\" If user choose Submit button, all values should be print which user has choose above.\n",
    "8. Lastly, I want this window generating code in one class, that is \"WindowGenerating\" class, I want other class name is \"Main\", I want to inherit \"WindowGenerating\" to \"main\" class, user should call \"main\" class and then window should appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d24ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2400fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2f2c40f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 75, 300])\n",
      "torch.Size([1, 128, 37, 150])\n",
      "torch.Size([1, 64, 37, 150])\n",
      "torch.Size([1, 64, 18, 75])\n",
      "torch.Size([1, 75, 64, 18])\n",
      "torch.Size([1, 75, 1152])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 20])\n",
      "torch.Size([75, 1, 20])\n",
      "torch.Size([75, 1, 20])\n",
      "tensor([75], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CaptchModel(nn.Module):\n",
    "    def __init__(self, num_char):\n",
    "        super(CaptchModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.fc1 = nn.Linear(1152, 64)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.lstm = nn.GRU(64, 32, bidirectional=True, dropout=0.25, batch_first=True)\n",
    "        self.fc2 = nn.Linear(64, num_char+1)\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        btch, channels, height, width = images.size()\n",
    "        x = self.conv1(images)\n",
    "        print(x.size())\n",
    "        x = self.pool1(x)\n",
    "        print(x.size())\n",
    "        x = self.conv2(x)\n",
    "        print(x.size())\n",
    "        x = self.pool2(x)\n",
    "        print(x.size())\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        print(x.size())\n",
    "        x = x.view(btch, x.size(1), -1)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.size())\n",
    "        x = self.drop(x)\n",
    "        print(x.size())\n",
    "        x, _ = self.lstm(x)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(x.size())\n",
    "        x = x.permute(1, 0, 2)\n",
    "        print(x.size())\n",
    "        if targets is not None:\n",
    "            log_prob = F.log_softmax(x, 2)\n",
    "            print(log_prob.size())\n",
    "            input_lengths = torch.full(size=(btch, ), fill_value=log_prob.size(0), dtype=torch.int32\n",
    "            )\n",
    "            print(input_lengths)\n",
    "        return x, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cm = CaptchModel(19)\n",
    "    images = torch.randn(1, 3, 75, 300)\n",
    "    targets = torch.randint(1, 20, (1, 19))\n",
    "    cm(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b0306136",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8544\\308123352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0moutput_seq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# output_seq_lengths[i] is the length of output_seq[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0minput_seq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input_seq_lengths[i] is the length of input_seq[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_seq_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_seq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1756\u001b[1;33m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[0;32m   1757\u001b[0m                           self.zero_infinity)\n\u001b[0;32m   1758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2629\u001b[0m             \u001b[0mblank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m         )\n\u001b[1;32m-> 2631\u001b[1;33m     return torch.ctc_loss(\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input and output sequences\n",
    "input_seq = torch.randn(32, 20, 80) # batch_size x input_seq_len x num_features\n",
    "output_seq = torch.randint(low=0, high=28, size=(32, 15)) # batch_size x output_seq_len\n",
    "\n",
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Compute the CTC loss\n",
    "output_seq_lengths = torch.full(size=(32,), fill_value=15, dtype=torch.int32) # output_seq_lengths[i] is the length of output_seq[i]\n",
    "input_seq_lengths = torch.full(size=(32,), fill_value=20, dtype=torch.int32) # input_seq_lengths[i] is the length of input_seq[i]\n",
    "loss = ctc_loss(input_seq, output_seq, output_seq_lengths, input_seq_lengths)\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "977fe834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 75, 300])\n",
      "torch.Size([1, 128, 37, 150])\n",
      "torch.Size([1, 64, 37, 150])\n",
      "torch.Size([1, 64, 18, 75])\n",
      "torch.Size([1, 75, 64, 18])\n",
      "torch.Size([1, 75, 1152])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 64])\n",
      "torch.Size([1, 75, 20])\n",
      "torch.Size([75, 1, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CaptchModel(nn.Module):\n",
    "    def __init__(self, num_char):\n",
    "        super(CaptchModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.fc1 = nn.Linear(1152, 64)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.GRU(64, 32, bidirectional=True, dropout=0.25, num_layers=2, batch_first=True)\n",
    "        self.fc2 = nn.Linear(64, num_char+1)\n",
    "    \n",
    "    def forward(self, images, targets):\n",
    "        btch, channels, height, width = images.size()\n",
    "        x = self.conv1(images)\n",
    "        print(x.size())\n",
    "        x = self.pool1(x)\n",
    "        print(x.size())\n",
    "        x = self.conv2(x)\n",
    "        print(x.size())\n",
    "        x = self.pool2(x)\n",
    "        print(x.size())\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        print(x.size())\n",
    "        x = x.view(btch, x.size(1), -1)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.size())\n",
    "        x = self.drop1(x)\n",
    "        print(x.size())\n",
    "        x, _ = self.lstm(x)\n",
    "        print(x.size())\n",
    "        x = self.fc2(x) \n",
    "        print(x.size())\n",
    "        x = x.permute(1, 0, 2)\n",
    "        print(x.size())\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(btch,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            target_lengths = torch.full(\n",
    "                size=(btch,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        return x, None\n",
    "\n",
    "cm = CaptchModel(19)\n",
    "images = torch.randn(1, 3, 75, 300)\n",
    "targets = torch.randint(1, 20, (1, 19))\n",
    "x, loss = cm(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73693247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa17ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e1632fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128, 75, 300])\n",
      "torch.Size([5, 128, 37, 150])\n",
      "torch.Size([5, 64, 37, 150])\n",
      "torch.Size([5, 64, 18, 75])\n",
      "torch.Size([5, 75, 64, 18])\n",
      "torch.Size([5, 75, 1152])\n",
      "torch.Size([5, 75, 64])\n",
      "torch.Size([5, 75, 64])\n",
      "torch.Size([5, 75, 64])\n",
      "torch.Size([5, 75, 20])\n",
      "torch.Size([75, 5, 20])\n",
      "tensor([75, 75, 75, 75, 75], dtype=torch.int32)\n",
      "tensor([5, 5, 5, 5, 5], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CaptchaModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CaptchaModel, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.linear_1 = nn.Linear(1152, 64)\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, _, _, _ = images.size()\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        print(x.size())\n",
    "        x = self.pool_1(x)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        print(x.size())\n",
    "        x = self.pool_2(x)\n",
    "        print(x.size())\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        print(x.size())\n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        print(x.size())\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        print(x.size())\n",
    "        x = self.drop_1(x)\n",
    "        print(x.size())\n",
    "        x, _ = self.lstm(x)\n",
    "        print(x.size())\n",
    "        x = self.output(x)\n",
    "        print(x.size())\n",
    "        x = x.permute(1, 0, 2)\n",
    "        print(x.size())\n",
    "\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            print(input_lengths)\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            print(target_lengths)\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "        \n",
    "        if targets is not None:\n",
    "            F.log_softmax\n",
    "        \n",
    "        return x, None\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cm = CaptchaModel(19)\n",
    "    img = torch.rand((5, 3, 75, 300))\n",
    "    targets = torch.randint(1, 20, (5, 5))\n",
    "    x, _ = cm(img, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d4f315c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randint(1, 20, (5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ee21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e9ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f6c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8419c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ed2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
